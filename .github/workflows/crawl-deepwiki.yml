name: Crawl DeepWiki Documentation

on:
  # Trigger on push to specific branches and paths
  push:
    branches: [main, develop]
    paths: ['.github/workflows/crawl-deepwiki.yml', 'docs/**']

  # Allow manual trigger
  workflow_dispatch:
    inputs:
      repo_identifier:
        description: 'Repository identifier (owner/repo or GitHub URL)'
        required: false
        default: ''
        type: string
      email:
        description: 'Email address for crawling authentication'
        required: true
        type: string

permissions:
  contents: read
  deployments: write

# Environment configuration
env:
  MAX_CONCURRENCY: ${{ vars.MAX_CONCURRENCY || '3' }}
  REQUEST_TIMEOUT: ${{ vars.REQUEST_TIMEOUT || '30000' }}
  RETRY_ATTEMPTS: ${{ vars.RETRY_ATTEMPTS || '3' }}
  USER_AGENT: 'DeepWiki-Crawler/1.0'

# Allow only one concurrent crawl
concurrency:
  group: "deepwiki-crawl"
  cancel-in-progress: false

jobs:
  crawl:
    environment:
      name: deepwiki
      url: ${{ steps.set-deployment-url.outputs.deepwiki_url }}
    runs-on: ubuntu-latest
    outputs:
      repo_exists: ${{ steps.check-repo.outputs.repo_exists }}
      org_repo: ${{ steps.check-repo.outputs.org_repo }}

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'

    - name: Install dependencies
      run: |
        npm install node-fetch@2.7.0 cheerio @types/node

    - name: Determine repository identifier
      id: repo-info
      run: |
        set -e

        # Input validation function
        validate_repo_id() {
          local repo_id="$1"
          # Allow only alphanumeric, hyphens, underscores, dots, and forward slashes
          if [[ ! "$repo_id" =~ ^[a-zA-Z0-9._/-]+$ ]] || [[ "$repo_id" =~ [\;\&\|\`\$\(\)\{\}\[\]\<\>] ]]; then
            echo "Error: Invalid repository identifier format" >&2
            exit 1
          fi
          # Validate format (owner/repo or URL)
          if [[ "$repo_id" =~ ^https://github\.com/[a-zA-Z0-9._-]+/[a-zA-Z0-9._-]+/?$ ]] ||
             [[ "$repo_id" =~ ^https://deepwiki\.com/[a-zA-Z0-9._-]+/[a-zA-Z0-9._-]+/?$ ]] ||
             [[ "$repo_id" =~ ^[a-zA-Z0-9._-]+/[a-zA-Z0-9._-]+$ ]]; then
            return 0
          else
            echo "Error: Repository identifier must be in format 'owner/repo' or valid GitHub/DeepWiki URL" >&2
            exit 1
          fi
        }

        if [ -n "${{ github.event.inputs.repo_identifier }}" ]; then
          REPO_ID="${{ github.event.inputs.repo_identifier }}"
          validate_repo_id "$REPO_ID"
          echo "repo_id=$REPO_ID" >> $GITHUB_OUTPUT
        else
          REPO_ID="${{ github.repository }}"
          validate_repo_id "$REPO_ID"
          echo "repo_id=$REPO_ID" >> $GITHUB_OUTPUT
        fi
        echo "Repository identifier validated successfully"

    - name: Set deployment URL
      id: set-deployment-url
      run: |
        set -e

        REPO_ID="${{ steps.repo-info.outputs.repo_id }}"

        # Extract org/repo from different formats with validation
        if [[ "$REPO_ID" == https://github.com/* ]]; then
          ORG_REPO=$(echo "$REPO_ID" | sed 's|https://github.com/||' | sed 's|\.git$||' | sed 's|/$||')
        elif [[ "$REPO_ID" == https://deepwiki.com/* ]]; then
          ORG_REPO=$(echo "$REPO_ID" | sed 's|https://deepwiki.com/||' | sed 's|/$||')
        else
          ORG_REPO="$REPO_ID"
        fi

        # Validate extracted org/repo format
        if [[ ! "$ORG_REPO" =~ ^[a-zA-Z0-9._-]+/[a-zA-Z0-9._-]+$ ]]; then
          echo "Error: Invalid org/repo format extracted: $ORG_REPO" >&2
          exit 1
        fi

        DEEPWIKI_URL="https://deepwiki.com/$ORG_REPO"
        echo "deepwiki_url=$DEEPWIKI_URL" >> $GITHUB_OUTPUT
        echo "üìç DeepWiki URL will be: $DEEPWIKI_URL"

    - name: Validate email input
      run: |
        set -e

        validate_email() {
          local email="$1"
          if [[ -n "$email" ]] && [[ ! "$email" =~ ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$ ]]; then
            echo "Error: Invalid email format" >&2
            exit 1
          fi
        }

        EMAIL="${{ github.event.inputs.email }}"
        if [ -n "$EMAIL" ]; then
          validate_email "$EMAIL"
          echo "Email format validated successfully"
        else
          echo "No email provided (triggered by push event)"
        fi

    - name: Check if repository exists on DeepWiki
      id: check-repo
      run: |
        set -e

        cat > check-deepwiki.js << 'EOF'
        const fetch = require('node-fetch');

        const REQUEST_TIMEOUT = parseInt(process.env.REQUEST_TIMEOUT) || 30000;
        const RETRY_ATTEMPTS = parseInt(process.env.RETRY_ATTEMPTS) || 3;
        const USER_AGENT = process.env.USER_AGENT || 'DeepWiki-Crawler/1.0';

        // Add proper error handling
        process.on('unhandledRejection', (reason, promise) => {
          console.error('Unhandled Rejection at:', promise, 'reason:', reason);
          process.exit(1);
        });

        // Retry logic with exponential backoff
        async function fetchWithRetry(url, options = {}, retries = RETRY_ATTEMPTS) {
          for (let i = 0; i < retries; i++) {
            try {
              const response = await fetch(url, {
                timeout: REQUEST_TIMEOUT,
                ...options,
                headers: {
                  'User-Agent': USER_AGENT,
                  ...options.headers
                }
              });

              if (response.status === 429) {
                const retryAfter = response.headers.get('Retry-After') || Math.pow(2, i) * 1000;
                console.log(`Rate limited. Waiting ${retryAfter}ms before retry...`);
                await new Promise(resolve => setTimeout(resolve, retryAfter));
                continue;
              }

              return response;
            } catch (error) {
              if (i === retries - 1) throw error;
              const delay = Math.pow(2, i) * 1000;
              console.log(`Request failed, retrying in ${delay}ms...`);
              await new Promise(resolve => setTimeout(resolve, delay));
            }
          }
        }

        async function checkRepoExistsOnDeepWiki(repoIdentifier) {
          // Extract org/repo from different input formats
          let orgRepo = '';

          if (repoIdentifier.startsWith('https://github.com/')) {
            const url = new URL(repoIdentifier);
            const pathParts = url.pathname.split('/').filter(Boolean);
            if (pathParts.length >= 2) {
              orgRepo = `${pathParts[0]}/${pathParts[1]}`;
            }
          } else if (repoIdentifier.startsWith('https://deepwiki.com/')) {
            const url = new URL(repoIdentifier);
            const pathParts = url.pathname.split('/').filter(Boolean);
            if (pathParts.length >= 2) {
              orgRepo = `${pathParts[0]}/${pathParts[1]}`;
            }
          } else if (repoIdentifier.includes('/')) {
            const parts = repoIdentifier.split('/');
            if (parts.length === 2 && parts[0] && parts[1]) {
              orgRepo = repoIdentifier;
            }
          }

          if (!orgRepo) {
            console.error('Invalid repository identifier format');
            process.exit(1);
          }

          console.log(`Checking if ${orgRepo} exists on DeepWiki...`);

          try {
            const searchUrl = `https://api.devin.ai/ada/list_public_indexes?search_repo=${encodeURIComponent(orgRepo)}`;
            const response = await fetchWithRetry(searchUrl, {
              headers: {
                accept: "*/*",
                "accept-language": "en,en-US;q=0.9",
              },
              method: "GET",
            });

            if (!response.ok) {
              console.error(`API error: ${response.status} ${response.statusText}`);
              process.exit(1);
            }

            const responseBodyText = await response.text();
            const parsedJson = JSON.parse(responseBodyText);

            if (parsedJson && Array.isArray(parsedJson.indices)) {
              const apiResults = parsedJson.indices;
              const repoExists = apiResults.some(repo => repo.repo_name === orgRepo);

              if (repoExists) {
                console.log(`‚úÖ Repository ${orgRepo} found on DeepWiki`);
                const fs = require('fs');
                fs.appendFileSync(process.env.GITHUB_OUTPUT, `repo_exists=true\n`);
                fs.appendFileSync(process.env.GITHUB_OUTPUT, `org_repo=${orgRepo}\n`);
              } else {
                console.log(`‚ùå Repository ${orgRepo} not found on DeepWiki`);
                console.log(`Will attempt to crawl and create documentation...`);
                const fs = require('fs');
                fs.appendFileSync(process.env.GITHUB_OUTPUT, `repo_exists=false\n`);
                fs.appendFileSync(process.env.GITHUB_OUTPUT, `org_repo=${orgRepo}\n`);
              }
            } else {
              console.error('Invalid API response format');
              process.exit(1);
            }
          } catch (error) {
            console.error('Error checking repository existence:', error.message);
            process.exit(1);
          }
        }

        const repoIdentifier = process.argv[2];
        const email = process.argv[3];
        checkRepoExistsOnDeepWiki(repoIdentifier);
        EOF

        EMAIL="${{ github.event.inputs.email }}"
        if [ -z "$EMAIL" ]; then
          echo "No email provided (triggered by push). Using default..."
          EMAIL="crawler@example.com"
        fi

        # Use parameterized execution to prevent command injection
        node check-deepwiki.js "${{ steps.repo-info.outputs.repo_id }}" "$EMAIL"

    - name: Create crawl script
      run: |
        cat > crawl-deepwiki.js << 'EOF'
        const fetch = require('node-fetch');
        const cheerio = require('cheerio');
        const fs = require('fs');
        const { URL } = require('url');

        // Configuration from environment variables
        const MAX_CONCURRENCY = parseInt(process.env.MAX_CONCURRENCY) || 3;
        const REQUEST_TIMEOUT = parseInt(process.env.REQUEST_TIMEOUT) || 30000;
        const RETRY_ATTEMPTS = parseInt(process.env.RETRY_ATTEMPTS) || 3;
        const USER_AGENT = process.env.USER_AGENT || 'DeepWiki-Crawler/1.0';

        // Queue class for better performance than array.shift()
        class Queue {
          constructor() {
            this.items = {};
            this.head = 0;
            this.tail = 0;
          }

          enqueue(item) {
            this.items[this.tail] = item;
            this.tail++;
          }

          dequeue() {
            if (this.isEmpty()) return null;
            const item = this.items[this.head];
            delete this.items[this.head];
            this.head++;
            return item;
          }

          isEmpty() {
            return this.head === this.tail;
          }

          size() {
            return this.tail - this.head;
          }
        }

        // Add proper error handling
        process.on('unhandledRejection', (reason, promise) => {
          console.error('Unhandled Rejection at:', promise, 'reason:', reason);
          process.exit(1);
        });

        // Retry logic with exponential backoff
        async function fetchWithRetry(url, options = {}, retries = RETRY_ATTEMPTS) {
          for (let i = 0; i < retries; i++) {
            try {
              const response = await fetch(url, {
                timeout: REQUEST_TIMEOUT,
                ...options,
                headers: {
                  'User-Agent': USER_AGENT,
                  ...options.headers
                }
              });

              if (response.status === 429) {
                const retryAfter = response.headers.get('Retry-After') || Math.pow(2, i) * 1000;
                console.log(`Rate limited. Waiting ${retryAfter}ms before retry...`);
                await new Promise(resolve => setTimeout(resolve, retryAfter));
                continue;
              }

              return response;
            } catch (error) {
              if (i === retries - 1) throw error;
              const delay = Math.pow(2, i) * 1000;
              console.log(`Request failed, retrying in ${delay}ms...`);
              await new Promise(resolve => setTimeout(resolve, delay));
            }
          }
        }

        function isValidHttpUrl(string) {
          let url;
          try {
            url = new URL(string);
          } catch (_) {
            return false;
          }
          return url.protocol === "http:" || url.protocol === "https:";
        }

        function getBaseUrl(repoIdentifier) {
          if (isValidHttpUrl(repoIdentifier)) {
            const url = new URL(repoIdentifier);
            if (url.hostname === "github.com" || url.hostname === "www.github.com") {
              const pathParts = url.pathname.split("/").filter(Boolean);
              if (pathParts.length >= 2) {
                return `https://deepwiki.com/${pathParts[0]}/${pathParts[1]}/`;
              }
            } else if (url.hostname === "deepwiki.com" || url.hostname === "www.deepwiki.com") {
              const pathParts = url.pathname.split("/").filter(Boolean);
              if (pathParts.length >= 2) {
                return `https://deepwiki.com/${pathParts[0]}/${pathParts[1]}/`;
              }
            }
          } else {
            const parts = repoIdentifier.split("/");
            if (parts.length === 2 && parts[0] && parts[1]) {
              return `https://deepwiki.com/${parts[0]}/${parts[1]}/`;
            }
          }
          return null;
        }

        async function crawlPage(url, baseUrl, visited, queue, writeStream, stats) {
          if (!url.startsWith(baseUrl)) {
            return;
          }

          console.log(`Crawling: ${url}`);

          let response = null;
          try {
            response = await fetchWithRetry(url);

            if (!response.ok) {
              stats.failed++;
              console.log(`Failed to fetch ${url}: ${response.status}`);
              return;
            }

            stats.successful++;
            const html = await response.text();
            const $ = cheerio.load(html);

            const contentSelector = "div.prose-custom";
            const pageContent = $(contentSelector).text().trim();
            if (pageContent) {
              // Stream content instead of storing in memory
              const contentChunk = `## Page: ${url}\n\n${pageContent}\n\n---\n\n`;
              writeStream.write(contentChunk);
            }

            $("a[href]").each((_, element) => {
              const link = $(element).attr("href");
              if (link) {
                try {
                  const absoluteUrl = new URL(link, url).toString();
                  const absoluteUrlWithoutHash = absoluteUrl.split("#")[0];

                  if (absoluteUrl.startsWith(baseUrl) && !visited.has(absoluteUrlWithoutHash)) {
                    queue.enqueue(absoluteUrl);
                  }
                } catch (e) {
                  // Skip invalid URLs
                }
              }
            });
          } catch (error) {
            stats.failed++;
            console.log(`Error crawling ${url}:`, error.message);
          }
        }

        async function crawlWorker(queue, visited, baseUrl, writeStream, stats, activeCount, progressTracker) {
          if (activeCount.current >= MAX_CONCURRENCY) {
            return;
          }
          activeCount.current++;

          try {
            let keepWorking = true;
            while (keepWorking) {
              const url = queue.dequeue();
              if (!url) {
                keepWorking = false;
                break;
              }

              const urlWithoutHash = url.split("#")[0];
              if (visited.has(urlWithoutHash)) {
                continue;
              }
              visited.add(urlWithoutHash);

              stats.attempted++;
              await crawlPage(url, baseUrl, visited, queue, writeStream, stats);

              // Update progress
              progressTracker.processed++;
              if (progressTracker.processed % 10 === 0) {
                console.log(`Progress: ${progressTracker.processed} pages processed, ${queue.size()} in queue`);
              }

              // Spawn additional workers if needed
              while (queue.size() > 0 && activeCount.current < MAX_CONCURRENCY) {
                setImmediate(() => crawlWorker(queue, visited, baseUrl, writeStream, stats, activeCount, progressTracker));
              }
            }
          } finally {
            activeCount.current--;
          }
        }

        async function main() {
          const repoIdentifier = process.argv[2];
          const email = process.argv[3];
          const baseUrl = getBaseUrl(repoIdentifier);

          if (!baseUrl) {
            console.error("Invalid repository identifier. Use format 'org/repo' or GitHub URL.");
            process.exit(1);
          }

          console.log(`Starting crawl for ${repoIdentifier} at ${baseUrl}`);

          const visited = new Set();
          const queue = new Queue();
          const activeCount = { current: 0 };
          const progressTracker = { processed: 0 };

          // Create write stream for memory-efficient content storage
          const writeStream = fs.createWriteStream('deepwiki-content.md');
          let hasContent = false;

          writeStream.on('error', (error) => {
            console.error('Write stream error:', error);
            process.exit(1);
          });

          // Override write to track if content was written
          const originalWrite = writeStream.write.bind(writeStream);
          writeStream.write = (chunk) => {
            hasContent = true;
            return originalWrite(chunk);
          };

          const crawlStats = {
            attempted: 0,
            successful: 0,
            failed: 0,
          };

          // Seed the queue
          queue.enqueue(baseUrl);

          // Start initial worker
          const workers = [];
          workers.push(crawlWorker(queue, visited, baseUrl, writeStream, crawlStats, activeCount, progressTracker));

          // Wait for completion
          while (queue.size() > 0 || activeCount.current > 0) {
            await new Promise(resolve => setTimeout(resolve, 100));
          }

          // Close the write stream
          writeStream.end();
          await new Promise(resolve => writeStream.on('finish', resolve));

          console.log(`Crawl complete. Attempted: ${crawlStats.attempted}, Successful: ${crawlStats.successful}, Failed: ${crawlStats.failed}`);

          if (hasContent && fs.existsSync('deepwiki-content.md') && fs.statSync('deepwiki-content.md').size > 0) {
            const finalContent = fs.readFileSync('deepwiki-content.md', 'utf8');

            // Create index.html with the content
            const htmlContent = `<!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>DeepWiki Documentation - ${repoIdentifier}</title>
            <style>
                body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; line-height: 1.6; max-width: 800px; margin: 0 auto; padding: 20px; }
                h2 { color: #0366d6; border-bottom: 1px solid #e1e4e8; padding-bottom: 10px; }
                hr { border: none; border-top: 1px solid #e1e4e8; margin: 2em 0; }
                pre { background: #f6f8fa; padding: 1em; border-radius: 6px; overflow-x: auto; }
                code { background: #f6f8fa; padding: 0.2em 0.4em; border-radius: 3px; }
                .header { text-align: center; margin-bottom: 2em; }
                .redirect-notice { background: #fff3cd; border: 1px solid #ffeaa7; border-radius: 6px; padding: 1em; margin-bottom: 2em; }
            </style>
        </head>
        <body>
            <div class="header">
                <h1>DeepWiki Documentation</h1>
                <p>Repository: <code>${repoIdentifier}</code></p>
                <div class="redirect-notice">
                    <strong>üìç This documentation is also available at:</strong><br>
                    <a href="https://deepwiki.com/${repoIdentifier.replace('https://github.com/', '').replace('https://deepwiki.com/', '')}" target="_blank">
                        https://deepwiki.com/${repoIdentifier.replace('https://github.com/', '').replace('https://deepwiki.com/', '')}
                    </a>
                </div>
            </div>
            <div id="content">
        ${finalContent.split('\n').map(line => {
          if (line.startsWith('## Page:')) {
            return `<h2>${line.substring(3)}</h2>`;
          } else if (line === '---') {
            return '<hr>';
          } else if (line.trim()) {
            return `<p>${line}</p>`;
          }
          return '';
        }).join('\n')}
            </div>
        </body>
        </html>`;

            fs.writeFileSync('index.html', htmlContent);
            fs.writeFileSync('deepwiki-content.md', finalContent);

            console.log('Generated index.html and deepwiki-content.md');
          } else {
            console.log('‚è≥ No content extracted immediately - this is normal!');
            console.log('üîÑ DeepWiki indexing takes time. The repository will be indexed eventually.');
            console.log(`üîó Check back later at: https://deepwiki.com/${repoIdentifier.replace('https://github.com/', '').replace('https://deepwiki.com/', '')}`);

            // Validate that we at least tried to crawl
            if (crawlStats.attempted === 0) {
              console.error('No pages were attempted to be crawled');
              process.exit(1);
            }
          }
        }

        main().catch(console.error);
        EOF

    - name: Run DeepWiki crawler
      run: |
        set -e

        EMAIL="${{ github.event.inputs.email }}"
        if [ -z "$EMAIL" ]; then
          echo "No email provided (triggered by push). Using default..."
          EMAIL="crawler@example.com"
        fi

        # Use parameterized execution to prevent command injection
        node crawl-deepwiki.js "${{ steps.repo-info.outputs.repo_id }}" "$EMAIL"

    - name: Show results summary
      run: |
        set -e

        echo "üéØ DeepWiki Crawl Results for ${{ steps.check-repo.outputs.org_repo }}"
        echo "üîó DeepWiki URL: https://deepwiki.com/${{ steps.check-repo.outputs.org_repo }}"
        echo ""

        if [ "${{ steps.check-repo.outputs.repo_exists }}" == "true" ]; then
          echo "‚úÖ Repository was already indexed on DeepWiki!"
          echo "üìö Documentation should be available immediately"
        else
          echo "üîÑ Repository indexing initiated on DeepWiki"
          if [ -f "deepwiki-content.md" ] && [ -s "deepwiki-content.md" ]; then
            echo "‚úÖ Some content was crawled successfully"
            echo "üìÑ Generated local documentation files"
            echo "üìä Content size: $(wc -c < deepwiki-content.md) bytes"
          else
            echo "‚è≥ No content extracted immediately (this is normal!)"
            echo "üïê DeepWiki indexing happens asynchronously and takes time"
          fi
          echo "üí° Check the DeepWiki URL above periodically - documentation will appear once indexing completes"
        fi

        # Cleanup temporary files
        rm -f check-deepwiki.js crawl-deepwiki.js
